{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc1ZEQM/bj4U15jU/YlFkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianshapojun/AGCRN/blob/master/Lab_AGCRN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0gCMKKi51A5",
        "outputId": "34f35b74-4ade-4fbe-b38b-b79813204954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tianshapojun/AGCRN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksTgdFY56AN2",
        "outputId": "bbe21181-fa9d-46d0-8291-ad52ab6bbb75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AGCRN'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 52 (delta 17), reused 3 (delta 3), pack-reused 21\u001b[K\n",
            "Unpacking objects: 100% (52/52), 20.10 KiB | 1.55 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import shutil\n",
        "#oldpath = '/content/gdrive/MyDrive/Data/TrafficFlow/Macro_Data/PEMSD4'\n",
        "#newpath = '/content/AGCRN/data/PeMSD4'\n",
        "#shutil.copytree(oldpath,newpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kQQ3Lq9-6cWj",
        "outputId": "6c54b3e5-ef30-46ba-d022-6edc5777d0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/AGCRN/data/PeMSD4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "oldpath = '/content/gdrive/MyDrive/Data/TrafficFlow/Macro_Data/Generated'\n",
        "newpath = '/content/AGCRN/data/Macro'\n",
        "shutil.copytree(oldpath,newpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hvftuHzezhZc",
        "outputId": "9cc7856e-5f9e-4548-9a77-3a380638377d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/AGCRN/data/Macro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AGCRN/model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm8lqQuP-QSK",
        "outputId": "89c3fc9a-211b-4c3a-c153-b4f3769178ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AGCRN/model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/AGCRN/model/Run.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYVthOfZ7kdq",
        "outputId": "4a815a82-b4e8-4fbd-cbf2-35deb2329093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AGCRN\n",
            "*****************Model Parameter*****************\n",
            "node_embeddings torch.Size([93, 10]) True\n",
            "encoder.dcrnn_cells.0.gate.weights_pool torch.Size([10, 2, 65, 128]) True\n",
            "encoder.dcrnn_cells.0.gate.bias_pool torch.Size([10, 128]) True\n",
            "encoder.dcrnn_cells.0.update.weights_pool torch.Size([10, 2, 65, 64]) True\n",
            "encoder.dcrnn_cells.0.update.bias_pool torch.Size([10, 64]) True\n",
            "encoder.dcrnn_cells.1.gate.weights_pool torch.Size([10, 2, 128, 128]) True\n",
            "encoder.dcrnn_cells.1.gate.bias_pool torch.Size([10, 128]) True\n",
            "encoder.dcrnn_cells.1.update.weights_pool torch.Size([10, 2, 128, 64]) True\n",
            "encoder.dcrnn_cells.1.update.bias_pool torch.Size([10, 64]) True\n",
            "end_conv.weight torch.Size([12, 1, 1, 64]) True\n",
            "end_conv.bias torch.Size([12]) True\n",
            "Total params num: 746670\n",
            "*****************Finish Parameter****************\n",
            "Load SAMPLES Dataset shaped:  (8640, 93, 1) 83.49783360748381 4.948726898264639 60.87448817649003 63.25236057691942\n",
            "Normalize the dataset by Standard Normalization\n",
            "Train:  (5161, 12, 93, 1) (5161, 12, 93, 1)\n",
            "Val:  (1705, 12, 93, 1) (1705, 12, 93, 1)\n",
            "Test:  (1705, 12, 93, 1) (1705, 12, 93, 1)\n",
            "Creat Log File in:  /content/gdrive/MyDrive/Models/AGCRN/experiments/Speed20230423023839/run.log\n",
            "2023-04-23 02:38: Experiment log path in: /content/gdrive/MyDrive/Models/AGCRN/experiments/Speed20230423023839\n",
            "2023-04-23 02:38: Train Epoch 1: 0/161 Loss: 60.513210\n",
            "2023-04-23 02:38: Train Epoch 1: 20/161 Loss: 56.568085\n",
            "2023-04-23 02:38: Train Epoch 1: 40/161 Loss: 52.678165\n",
            "2023-04-23 02:38: Train Epoch 1: 60/161 Loss: 46.563911\n",
            "2023-04-23 02:38: Train Epoch 1: 80/161 Loss: 43.338802\n",
            "2023-04-23 02:38: Train Epoch 1: 100/161 Loss: 38.467846\n",
            "2023-04-23 02:38: Train Epoch 1: 120/161 Loss: 33.990002\n",
            "2023-04-23 02:38: Train Epoch 1: 140/161 Loss: 30.335091\n",
            "2023-04-23 02:38: Train Epoch 1: 160/161 Loss: 27.022989\n",
            "2023-04-23 02:38: **********Train Epoch 1: averaged Loss: 43.219952, tf_ratio: 1.000000\n",
            "2023-04-23 02:38: **********Val Epoch 1: average Loss: 25.809068\n",
            "2023-04-23 02:38: *********************************Current best model saved!\n",
            "2023-04-23 02:38: Train Epoch 2: 0/161 Loss: 25.586542\n",
            "2023-04-23 02:38: Train Epoch 2: 20/161 Loss: 22.608311\n",
            "2023-04-23 02:38: Train Epoch 2: 40/161 Loss: 18.700199\n",
            "2023-04-23 02:39: Train Epoch 2: 60/161 Loss: 16.147926\n",
            "2023-04-23 02:39: Train Epoch 2: 80/161 Loss: 13.097439\n",
            "2023-04-23 02:39: Train Epoch 2: 100/161 Loss: 10.691088\n",
            "2023-04-23 02:39: Train Epoch 2: 120/161 Loss: 7.223738\n",
            "2023-04-23 02:39: Train Epoch 2: 140/161 Loss: 6.240168\n",
            "2023-04-23 02:39: Train Epoch 2: 160/161 Loss: 5.496093\n",
            "2023-04-23 02:39: **********Train Epoch 2: averaged Loss: 13.494738, tf_ratio: 1.000000\n",
            "2023-04-23 02:39: **********Val Epoch 2: average Loss: 5.288962\n",
            "2023-04-23 02:39: *********************************Current best model saved!\n",
            "2023-04-23 02:39: Train Epoch 3: 0/161 Loss: 5.708828\n",
            "2023-04-23 02:39: Train Epoch 3: 20/161 Loss: 5.758959\n",
            "2023-04-23 02:39: Train Epoch 3: 40/161 Loss: 4.343431\n",
            "2023-04-23 02:39: Train Epoch 3: 60/161 Loss: 4.021361\n",
            "2023-04-23 02:39: Train Epoch 3: 80/161 Loss: 3.524932\n",
            "2023-04-23 02:39: Train Epoch 3: 100/161 Loss: 2.910221\n",
            "2023-04-23 02:39: Train Epoch 3: 120/161 Loss: 2.419940\n",
            "2023-04-23 02:39: Train Epoch 3: 140/161 Loss: 1.893960\n",
            "2023-04-23 02:39: Train Epoch 3: 160/161 Loss: 1.646404\n",
            "2023-04-23 02:39: **********Train Epoch 3: averaged Loss: 3.644231, tf_ratio: 1.000000\n",
            "2023-04-23 02:39: **********Val Epoch 3: average Loss: 1.694588\n",
            "2023-04-23 02:39: *********************************Current best model saved!\n",
            "2023-04-23 02:39: Train Epoch 4: 0/161 Loss: 1.689092\n",
            "2023-04-23 02:39: Train Epoch 4: 20/161 Loss: 1.403452\n",
            "2023-04-23 02:39: Train Epoch 4: 40/161 Loss: 1.081271\n",
            "2023-04-23 02:39: Train Epoch 4: 60/161 Loss: 0.774690\n",
            "2023-04-23 02:39: Train Epoch 4: 80/161 Loss: 0.909135\n",
            "2023-04-23 02:39: Train Epoch 4: 100/161 Loss: 0.753522\n",
            "2023-04-23 02:39: Train Epoch 4: 120/161 Loss: 0.751421\n",
            "2023-04-23 02:39: Train Epoch 4: 140/161 Loss: 0.787970\n",
            "2023-04-23 02:39: Train Epoch 4: 160/161 Loss: 0.703113\n",
            "2023-04-23 02:39: **********Train Epoch 4: averaged Loss: 0.978728, tf_ratio: 1.000000\n",
            "2023-04-23 02:39: **********Val Epoch 4: average Loss: 0.766298\n",
            "2023-04-23 02:39: *********************************Current best model saved!\n",
            "2023-04-23 02:39: Train Epoch 5: 0/161 Loss: 0.758841\n",
            "2023-04-23 02:39: Train Epoch 5: 20/161 Loss: 0.756987\n",
            "2023-04-23 02:39: Train Epoch 5: 40/161 Loss: 0.861771\n",
            "2023-04-23 02:39: Train Epoch 5: 60/161 Loss: 0.768298\n",
            "2023-04-23 02:39: Train Epoch 5: 80/161 Loss: 0.692585\n",
            "2023-04-23 02:39: Train Epoch 5: 100/161 Loss: 0.660360\n",
            "2023-04-23 02:39: Train Epoch 5: 120/161 Loss: 0.661804\n",
            "2023-04-23 02:39: Train Epoch 5: 140/161 Loss: 0.605378\n",
            "2023-04-23 02:39: Train Epoch 5: 160/161 Loss: 0.664972\n",
            "2023-04-23 02:39: **********Train Epoch 5: averaged Loss: 0.721129, tf_ratio: 1.000000\n",
            "2023-04-23 02:39: **********Val Epoch 5: average Loss: 0.744116\n",
            "2023-04-23 02:39: *********************************Current best model saved!\n",
            "2023-04-23 02:39: Train Epoch 6: 0/161 Loss: 0.700415\n",
            "2023-04-23 02:39: Train Epoch 6: 20/161 Loss: 0.658853\n",
            "2023-04-23 02:39: Train Epoch 6: 40/161 Loss: 0.664036\n",
            "2023-04-23 02:39: Train Epoch 6: 60/161 Loss: 0.700474\n",
            "2023-04-23 02:39: Train Epoch 6: 80/161 Loss: 0.655099\n",
            "2023-04-23 02:39: Train Epoch 6: 100/161 Loss: 0.657855\n",
            "2023-04-23 02:40: Train Epoch 6: 120/161 Loss: 0.670984\n",
            "2023-04-23 02:40: Train Epoch 6: 140/161 Loss: 0.697679\n",
            "2023-04-23 02:40: Train Epoch 6: 160/161 Loss: 0.661223\n",
            "2023-04-23 02:40: **********Train Epoch 6: averaged Loss: 0.660696, tf_ratio: 1.000000\n",
            "2023-04-23 02:40: **********Val Epoch 6: average Loss: 0.666963\n",
            "2023-04-23 02:40: *********************************Current best model saved!\n",
            "2023-04-23 02:40: Train Epoch 7: 0/161 Loss: 0.623062\n",
            "2023-04-23 02:40: Train Epoch 7: 20/161 Loss: 0.592964\n",
            "2023-04-23 02:40: Train Epoch 7: 40/161 Loss: 0.647140\n",
            "2023-04-23 02:40: Train Epoch 7: 60/161 Loss: 0.628116\n",
            "2023-04-23 02:40: Train Epoch 7: 80/161 Loss: 0.616972\n",
            "2023-04-23 02:40: Train Epoch 7: 100/161 Loss: 0.535551\n",
            "2023-04-23 02:40: Train Epoch 7: 120/161 Loss: 0.585526\n",
            "2023-04-23 02:40: Train Epoch 7: 140/161 Loss: 0.621759\n",
            "2023-04-23 02:40: Train Epoch 7: 160/161 Loss: 0.582333\n",
            "2023-04-23 02:40: **********Train Epoch 7: averaged Loss: 0.627584, tf_ratio: 1.000000\n",
            "2023-04-23 02:40: **********Val Epoch 7: average Loss: 0.623079\n",
            "2023-04-23 02:40: *********************************Current best model saved!\n",
            "2023-04-23 02:40: Train Epoch 8: 0/161 Loss: 0.677787\n",
            "2023-04-23 02:40: Train Epoch 8: 20/161 Loss: 0.548884\n",
            "2023-04-23 02:40: Train Epoch 8: 40/161 Loss: 0.632913\n",
            "2023-04-23 02:40: Train Epoch 8: 60/161 Loss: 0.726714\n",
            "2023-04-23 02:40: Train Epoch 8: 80/161 Loss: 0.580504\n",
            "2023-04-23 02:40: Train Epoch 8: 100/161 Loss: 0.620413\n",
            "2023-04-23 02:40: Train Epoch 8: 120/161 Loss: 0.574650\n",
            "2023-04-23 02:40: Train Epoch 8: 140/161 Loss: 0.578399\n",
            "2023-04-23 02:40: Train Epoch 8: 160/161 Loss: 0.682154\n",
            "2023-04-23 02:40: **********Train Epoch 8: averaged Loss: 0.596195, tf_ratio: 1.000000\n",
            "2023-04-23 02:40: **********Val Epoch 8: average Loss: 0.596806\n",
            "2023-04-23 02:40: *********************************Current best model saved!\n",
            "2023-04-23 02:40: Train Epoch 9: 0/161 Loss: 0.613402\n",
            "2023-04-23 02:40: Train Epoch 9: 20/161 Loss: 0.579285\n",
            "2023-04-23 02:40: Train Epoch 9: 40/161 Loss: 0.552550\n",
            "2023-04-23 02:40: Train Epoch 9: 60/161 Loss: 0.602520\n",
            "2023-04-23 02:40: Train Epoch 9: 80/161 Loss: 0.593321\n",
            "2023-04-23 02:40: Train Epoch 9: 100/161 Loss: 0.527631\n",
            "2023-04-23 02:40: Train Epoch 9: 120/161 Loss: 0.567439\n",
            "2023-04-23 02:40: Train Epoch 9: 140/161 Loss: 0.576143\n",
            "2023-04-23 02:40: Train Epoch 9: 160/161 Loss: 0.530423\n",
            "2023-04-23 02:40: **********Train Epoch 9: averaged Loss: 0.556985, tf_ratio: 1.000000\n",
            "2023-04-23 02:40: **********Val Epoch 9: average Loss: 0.594805\n",
            "2023-04-23 02:40: *********************************Current best model saved!\n",
            "2023-04-23 02:40: Train Epoch 10: 0/161 Loss: 0.566155\n",
            "2023-04-23 02:40: Train Epoch 10: 20/161 Loss: 0.537999\n",
            "2023-04-23 02:40: Train Epoch 10: 40/161 Loss: 0.492322\n",
            "2023-04-23 02:40: Train Epoch 10: 60/161 Loss: 0.522088\n",
            "2023-04-23 02:40: Train Epoch 10: 80/161 Loss: 0.469271\n",
            "2023-04-23 02:40: Train Epoch 10: 100/161 Loss: 0.613226\n",
            "2023-04-23 02:40: Train Epoch 10: 120/161 Loss: 0.485852\n",
            "2023-04-23 02:40: Train Epoch 10: 140/161 Loss: 0.491914\n",
            "2023-04-23 02:40: Train Epoch 10: 160/161 Loss: 0.491105\n",
            "2023-04-23 02:40: **********Train Epoch 10: averaged Loss: 0.521185, tf_ratio: 1.000000\n",
            "2023-04-23 02:40: **********Val Epoch 10: average Loss: 0.518744\n",
            "2023-04-23 02:40: *********************************Current best model saved!\n",
            "2023-04-23 02:40: Train Epoch 11: 0/161 Loss: 0.503938\n",
            "2023-04-23 02:41: Train Epoch 11: 20/161 Loss: 0.471700\n",
            "2023-04-23 02:41: Train Epoch 11: 40/161 Loss: 0.484774\n",
            "2023-04-23 02:41: Train Epoch 11: 60/161 Loss: 0.487120\n",
            "2023-04-23 02:41: Train Epoch 11: 80/161 Loss: 0.526151\n",
            "2023-04-23 02:41: Train Epoch 11: 100/161 Loss: 0.456233\n",
            "2023-04-23 02:41: Train Epoch 11: 120/161 Loss: 0.449228\n",
            "2023-04-23 02:41: Train Epoch 11: 140/161 Loss: 0.495986\n",
            "2023-04-23 02:41: Train Epoch 11: 160/161 Loss: 0.527403\n",
            "2023-04-23 02:41: **********Train Epoch 11: averaged Loss: 0.489367, tf_ratio: 1.000000\n",
            "2023-04-23 02:41: **********Val Epoch 11: average Loss: 0.527252\n",
            "2023-04-23 02:41: Train Epoch 12: 0/161 Loss: 0.518827\n",
            "2023-04-23 02:41: Train Epoch 12: 20/161 Loss: 0.469188\n",
            "2023-04-23 02:41: Train Epoch 12: 40/161 Loss: 0.504906\n",
            "2023-04-23 02:41: Train Epoch 12: 60/161 Loss: 0.487279\n",
            "2023-04-23 02:41: Train Epoch 12: 80/161 Loss: 0.436864\n",
            "2023-04-23 02:41: Train Epoch 12: 100/161 Loss: 0.419365\n",
            "2023-04-23 02:41: Train Epoch 12: 120/161 Loss: 0.483481\n",
            "2023-04-23 02:41: Train Epoch 12: 140/161 Loss: 0.441871\n",
            "2023-04-23 02:41: Train Epoch 12: 160/161 Loss: 0.424237\n",
            "2023-04-23 02:41: **********Train Epoch 12: averaged Loss: 0.457187, tf_ratio: 1.000000\n",
            "2023-04-23 02:41: **********Val Epoch 12: average Loss: 0.467107\n",
            "2023-04-23 02:41: *********************************Current best model saved!\n",
            "2023-04-23 02:41: Train Epoch 13: 0/161 Loss: 0.458346\n",
            "2023-04-23 02:41: Train Epoch 13: 20/161 Loss: 0.462708\n",
            "2023-04-23 02:41: Train Epoch 13: 40/161 Loss: 0.421371\n",
            "2023-04-23 02:41: Train Epoch 13: 60/161 Loss: 0.406985\n",
            "2023-04-23 02:41: Train Epoch 13: 80/161 Loss: 0.438380\n",
            "2023-04-23 02:41: Train Epoch 13: 100/161 Loss: 0.400349\n",
            "2023-04-23 02:41: Train Epoch 13: 120/161 Loss: 0.448115\n",
            "2023-04-23 02:41: Train Epoch 13: 140/161 Loss: 0.391616\n",
            "2023-04-23 02:41: Train Epoch 13: 160/161 Loss: 0.408819\n",
            "2023-04-23 02:41: **********Train Epoch 13: averaged Loss: 0.430311, tf_ratio: 1.000000\n",
            "2023-04-23 02:41: **********Val Epoch 13: average Loss: 0.457731\n",
            "2023-04-23 02:41: *********************************Current best model saved!\n",
            "2023-04-23 02:41: Train Epoch 14: 0/161 Loss: 0.418804\n",
            "2023-04-23 02:41: Train Epoch 14: 20/161 Loss: 0.421232\n",
            "2023-04-23 02:41: Train Epoch 14: 40/161 Loss: 0.373729\n",
            "2023-04-23 02:41: Train Epoch 14: 60/161 Loss: 0.462931\n",
            "2023-04-23 02:41: Train Epoch 14: 80/161 Loss: 0.397207\n",
            "2023-04-23 02:41: Train Epoch 14: 100/161 Loss: 0.384313\n",
            "2023-04-23 02:41: Train Epoch 14: 120/161 Loss: 0.395872\n",
            "2023-04-23 02:41: Train Epoch 14: 140/161 Loss: 0.411548\n",
            "2023-04-23 02:41: Train Epoch 14: 160/161 Loss: 0.398246\n",
            "2023-04-23 02:41: **********Train Epoch 14: averaged Loss: 0.415161, tf_ratio: 1.000000\n",
            "2023-04-23 02:41: **********Val Epoch 14: average Loss: 0.431815\n",
            "2023-04-23 02:41: *********************************Current best model saved!\n",
            "2023-04-23 02:41: Train Epoch 15: 0/161 Loss: 0.397666\n",
            "2023-04-23 02:41: Train Epoch 15: 20/161 Loss: 0.405651\n",
            "2023-04-23 02:41: Train Epoch 15: 40/161 Loss: 0.373558\n",
            "2023-04-23 02:41: Train Epoch 15: 60/161 Loss: 0.371780\n",
            "2023-04-23 02:42: Train Epoch 15: 80/161 Loss: 0.384416\n",
            "2023-04-23 02:42: Train Epoch 15: 100/161 Loss: 0.419876\n",
            "2023-04-23 02:42: Train Epoch 15: 120/161 Loss: 0.389512\n",
            "2023-04-23 02:42: Train Epoch 15: 140/161 Loss: 0.426768\n",
            "2023-04-23 02:42: Train Epoch 15: 160/161 Loss: 0.385987\n",
            "2023-04-23 02:42: **********Train Epoch 15: averaged Loss: 0.394045, tf_ratio: 1.000000\n",
            "2023-04-23 02:42: **********Val Epoch 15: average Loss: 0.416119\n",
            "2023-04-23 02:42: *********************************Current best model saved!\n",
            "2023-04-23 02:42: Train Epoch 16: 0/161 Loss: 0.409871\n",
            "2023-04-23 02:42: Train Epoch 16: 20/161 Loss: 0.411953\n",
            "2023-04-23 02:42: Train Epoch 16: 40/161 Loss: 0.401042\n",
            "2023-04-23 02:42: Train Epoch 16: 60/161 Loss: 0.370620\n",
            "2023-04-23 02:42: Train Epoch 16: 80/161 Loss: 0.366014\n",
            "2023-04-23 02:42: Train Epoch 16: 100/161 Loss: 0.362087\n",
            "2023-04-23 02:42: Train Epoch 16: 120/161 Loss: 0.389303\n",
            "2023-04-23 02:42: Train Epoch 16: 140/161 Loss: 0.381258\n",
            "2023-04-23 02:42: Train Epoch 16: 160/161 Loss: 0.367862\n",
            "2023-04-23 02:42: **********Train Epoch 16: averaged Loss: 0.380724, tf_ratio: 1.000000\n",
            "2023-04-23 02:42: **********Val Epoch 16: average Loss: 0.402038\n",
            "2023-04-23 02:42: *********************************Current best model saved!\n",
            "2023-04-23 02:42: Train Epoch 17: 0/161 Loss: 0.360147\n",
            "2023-04-23 02:42: Train Epoch 17: 20/161 Loss: 0.407064\n",
            "2023-04-23 02:42: Train Epoch 17: 40/161 Loss: 0.382738\n",
            "2023-04-23 02:42: Train Epoch 17: 60/161 Loss: 0.381993\n",
            "2023-04-23 02:42: Train Epoch 17: 80/161 Loss: 0.369537\n",
            "2023-04-23 02:42: Train Epoch 17: 100/161 Loss: 0.367541\n",
            "2023-04-23 02:42: Train Epoch 17: 120/161 Loss: 0.412891\n",
            "2023-04-23 02:42: Train Epoch 17: 140/161 Loss: 0.394602\n",
            "2023-04-23 02:42: Train Epoch 17: 160/161 Loss: 0.358507\n",
            "2023-04-23 02:42: **********Train Epoch 17: averaged Loss: 0.377363, tf_ratio: 1.000000\n",
            "2023-04-23 02:42: **********Val Epoch 17: average Loss: 0.394836\n",
            "2023-04-23 02:42: *********************************Current best model saved!\n",
            "2023-04-23 02:42: Train Epoch 18: 0/161 Loss: 0.353012\n",
            "2023-04-23 02:42: Train Epoch 18: 20/161 Loss: 0.355766\n",
            "2023-04-23 02:42: Train Epoch 18: 40/161 Loss: 0.366502\n",
            "2023-04-23 02:42: Train Epoch 18: 60/161 Loss: 0.392692\n",
            "2023-04-23 02:42: Train Epoch 18: 80/161 Loss: 0.383755\n",
            "2023-04-23 02:42: Train Epoch 18: 100/161 Loss: 0.385101\n",
            "2023-04-23 02:42: Train Epoch 18: 120/161 Loss: 0.342527\n",
            "2023-04-23 02:42: Train Epoch 18: 140/161 Loss: 0.386248\n",
            "2023-04-23 02:42: Train Epoch 18: 160/161 Loss: 0.354770\n",
            "2023-04-23 02:42: **********Train Epoch 18: averaged Loss: 0.364338, tf_ratio: 1.000000\n",
            "2023-04-23 02:42: **********Val Epoch 18: average Loss: 0.409906\n",
            "2023-04-23 02:42: Train Epoch 19: 0/161 Loss: 0.393088\n",
            "2023-04-23 02:42: Train Epoch 19: 20/161 Loss: 0.370264\n",
            "2023-04-23 02:42: Train Epoch 19: 40/161 Loss: 0.353797\n",
            "2023-04-23 02:42: Train Epoch 19: 60/161 Loss: 0.382739\n",
            "2023-04-23 02:42: Train Epoch 19: 80/161 Loss: 0.357803\n",
            "2023-04-23 02:42: Train Epoch 19: 100/161 Loss: 0.355657\n",
            "2023-04-23 02:42: Train Epoch 19: 120/161 Loss: 0.348655\n",
            "2023-04-23 02:43: Train Epoch 19: 140/161 Loss: 0.341596\n",
            "2023-04-23 02:43: Train Epoch 19: 160/161 Loss: 0.353245\n",
            "2023-04-23 02:43: **********Train Epoch 19: averaged Loss: 0.356568, tf_ratio: 1.000000\n",
            "2023-04-23 02:43: **********Val Epoch 19: average Loss: 0.386079\n",
            "2023-04-23 02:43: *********************************Current best model saved!\n",
            "2023-04-23 02:43: Train Epoch 20: 0/161 Loss: 0.348347\n",
            "2023-04-23 02:43: Train Epoch 20: 20/161 Loss: 0.381087\n",
            "2023-04-23 02:43: Train Epoch 20: 40/161 Loss: 0.303866\n",
            "2023-04-23 02:43: Train Epoch 20: 60/161 Loss: 0.343029\n",
            "2023-04-23 02:43: Train Epoch 20: 80/161 Loss: 0.345696\n",
            "2023-04-23 02:43: Train Epoch 20: 100/161 Loss: 0.343212\n",
            "2023-04-23 02:43: Train Epoch 20: 120/161 Loss: 0.353041\n",
            "2023-04-23 02:43: Train Epoch 20: 140/161 Loss: 0.343108\n",
            "2023-04-23 02:43: Train Epoch 20: 160/161 Loss: 0.344020\n",
            "2023-04-23 02:43: **********Train Epoch 20: averaged Loss: 0.345836, tf_ratio: 1.000000\n",
            "2023-04-23 02:43: **********Val Epoch 20: average Loss: 0.405702\n",
            "2023-04-23 02:43: Train Epoch 21: 0/161 Loss: 0.345669\n",
            "2023-04-23 02:43: Train Epoch 21: 20/161 Loss: 0.356419\n",
            "2023-04-23 02:43: Train Epoch 21: 40/161 Loss: 0.342403\n",
            "2023-04-23 02:43: Train Epoch 21: 60/161 Loss: 0.367713\n",
            "2023-04-23 02:43: Train Epoch 21: 80/161 Loss: 0.332584\n",
            "2023-04-23 02:43: Train Epoch 21: 100/161 Loss: 0.347964\n",
            "2023-04-23 02:43: Train Epoch 21: 120/161 Loss: 0.346673\n",
            "2023-04-23 02:43: Train Epoch 21: 140/161 Loss: 0.352168\n",
            "2023-04-23 02:43: Train Epoch 21: 160/161 Loss: 0.338598\n",
            "2023-04-23 02:43: **********Train Epoch 21: averaged Loss: 0.346664, tf_ratio: 1.000000\n",
            "2023-04-23 02:43: **********Val Epoch 21: average Loss: 0.375778\n",
            "2023-04-23 02:43: *********************************Current best model saved!\n",
            "2023-04-23 02:43: Train Epoch 22: 0/161 Loss: 0.361569\n",
            "2023-04-23 02:43: Train Epoch 22: 20/161 Loss: 0.348900\n",
            "2023-04-23 02:43: Train Epoch 22: 40/161 Loss: 0.339712\n",
            "2023-04-23 02:43: Train Epoch 22: 60/161 Loss: 0.353746\n",
            "2023-04-23 02:43: Train Epoch 22: 80/161 Loss: 0.318728\n",
            "2023-04-23 02:43: Train Epoch 22: 100/161 Loss: 0.336781\n",
            "2023-04-23 02:43: Train Epoch 22: 120/161 Loss: 0.331354\n",
            "2023-04-23 02:43: Train Epoch 22: 140/161 Loss: 0.333703\n",
            "2023-04-23 02:43: Train Epoch 22: 160/161 Loss: 0.344016\n",
            "2023-04-23 02:43: **********Train Epoch 22: averaged Loss: 0.335794, tf_ratio: 1.000000\n",
            "2023-04-23 02:43: **********Val Epoch 22: average Loss: 0.378015\n",
            "2023-04-23 02:43: Train Epoch 23: 0/161 Loss: 0.296350\n",
            "2023-04-23 02:43: Train Epoch 23: 20/161 Loss: 0.364663\n",
            "2023-04-23 02:43: Train Epoch 23: 40/161 Loss: 0.370538\n",
            "2023-04-23 02:43: Train Epoch 23: 60/161 Loss: 0.313315\n",
            "2023-04-23 02:43: Train Epoch 23: 80/161 Loss: 0.327681\n",
            "2023-04-23 02:43: Train Epoch 23: 100/161 Loss: 0.359442\n",
            "2023-04-23 02:43: Train Epoch 23: 120/161 Loss: 0.335520\n",
            "2023-04-23 02:43: Train Epoch 23: 140/161 Loss: 0.333951\n",
            "2023-04-23 02:43: Train Epoch 23: 160/161 Loss: 0.366158\n",
            "2023-04-23 02:43: **********Train Epoch 23: averaged Loss: 0.334361, tf_ratio: 1.000000\n",
            "2023-04-23 02:43: **********Val Epoch 23: average Loss: 0.410144\n",
            "2023-04-23 02:43: Train Epoch 24: 0/161 Loss: 0.383227\n",
            "2023-04-23 02:44: Train Epoch 24: 20/161 Loss: 0.345714\n",
            "2023-04-23 02:44: Train Epoch 24: 40/161 Loss: 0.363707\n",
            "2023-04-23 02:44: Train Epoch 24: 60/161 Loss: 0.319461\n",
            "2023-04-23 02:44: Train Epoch 24: 80/161 Loss: 0.321721\n",
            "2023-04-23 02:44: Train Epoch 24: 100/161 Loss: 0.310903\n",
            "2023-04-23 02:44: Train Epoch 24: 120/161 Loss: 0.328589\n",
            "2023-04-23 02:44: Train Epoch 24: 140/161 Loss: 0.317678\n",
            "2023-04-23 02:44: Train Epoch 24: 160/161 Loss: 0.341059\n",
            "2023-04-23 02:44: **********Train Epoch 24: averaged Loss: 0.325343, tf_ratio: 1.000000\n",
            "2023-04-23 02:44: **********Val Epoch 24: average Loss: 0.370853\n",
            "2023-04-23 02:44: *********************************Current best model saved!\n",
            "2023-04-23 02:44: Train Epoch 25: 0/161 Loss: 0.333154\n",
            "2023-04-23 02:44: Train Epoch 25: 20/161 Loss: 0.291691\n",
            "2023-04-23 02:44: Train Epoch 25: 40/161 Loss: 0.327760\n",
            "2023-04-23 02:44: Train Epoch 25: 60/161 Loss: 0.340215\n",
            "2023-04-23 02:44: Train Epoch 25: 80/161 Loss: 0.323655\n",
            "2023-04-23 02:44: Train Epoch 25: 100/161 Loss: 0.304617\n",
            "2023-04-23 02:44: Train Epoch 25: 120/161 Loss: 0.346483\n",
            "2023-04-23 02:44: Train Epoch 25: 140/161 Loss: 0.328888\n",
            "2023-04-23 02:44: Train Epoch 25: 160/161 Loss: 0.348795\n",
            "2023-04-23 02:44: **********Train Epoch 25: averaged Loss: 0.321517, tf_ratio: 1.000000\n",
            "2023-04-23 02:44: **********Val Epoch 25: average Loss: 0.369121\n",
            "2023-04-23 02:44: *********************************Current best model saved!\n",
            "2023-04-23 02:44: Train Epoch 26: 0/161 Loss: 0.306313\n",
            "2023-04-23 02:44: Train Epoch 26: 20/161 Loss: 0.306246\n",
            "2023-04-23 02:44: Train Epoch 26: 40/161 Loss: 0.341070\n",
            "2023-04-23 02:44: Train Epoch 26: 60/161 Loss: 0.319737\n",
            "2023-04-23 02:44: Train Epoch 26: 80/161 Loss: 0.308734\n",
            "2023-04-23 02:44: Train Epoch 26: 100/161 Loss: 0.307102\n",
            "2023-04-23 02:44: Train Epoch 26: 120/161 Loss: 0.305525\n",
            "2023-04-23 02:44: Train Epoch 26: 140/161 Loss: 0.318604\n",
            "2023-04-23 02:44: Train Epoch 26: 160/161 Loss: 0.359196\n",
            "2023-04-23 02:44: **********Train Epoch 26: averaged Loss: 0.318523, tf_ratio: 1.000000\n",
            "2023-04-23 02:44: **********Val Epoch 26: average Loss: 0.378438\n",
            "2023-04-23 02:44: Train Epoch 27: 0/161 Loss: 0.314347\n",
            "2023-04-23 02:44: Train Epoch 27: 20/161 Loss: 0.313618\n",
            "2023-04-23 02:44: Train Epoch 27: 40/161 Loss: 0.327399\n",
            "2023-04-23 02:44: Train Epoch 27: 60/161 Loss: 0.337126\n",
            "2023-04-23 02:44: Train Epoch 27: 80/161 Loss: 0.324779\n",
            "2023-04-23 02:44: Train Epoch 27: 100/161 Loss: 0.308123\n",
            "2023-04-23 02:44: Train Epoch 27: 120/161 Loss: 0.345008\n",
            "2023-04-23 02:44: Train Epoch 27: 140/161 Loss: 0.319844\n",
            "2023-04-23 02:44: Train Epoch 27: 160/161 Loss: 0.336628\n",
            "2023-04-23 02:44: **********Train Epoch 27: averaged Loss: 0.315425, tf_ratio: 1.000000\n",
            "2023-04-23 02:44: **********Val Epoch 27: average Loss: 0.379670\n",
            "2023-04-23 02:44: Train Epoch 28: 0/161 Loss: 0.329604\n",
            "2023-04-23 02:44: Train Epoch 28: 20/161 Loss: 0.348302\n",
            "2023-04-23 02:44: Train Epoch 28: 40/161 Loss: 0.334228\n",
            "2023-04-23 02:44: Train Epoch 28: 60/161 Loss: 0.285537\n",
            "2023-04-23 02:44: Train Epoch 28: 80/161 Loss: 0.315042\n",
            "2023-04-23 02:45: Train Epoch 28: 100/161 Loss: 0.304923\n",
            "2023-04-23 02:45: Train Epoch 28: 120/161 Loss: 0.300589\n",
            "2023-04-23 02:45: Train Epoch 28: 140/161 Loss: 0.298914\n",
            "2023-04-23 02:45: Train Epoch 28: 160/161 Loss: 0.302512\n",
            "2023-04-23 02:45: **********Train Epoch 28: averaged Loss: 0.310085, tf_ratio: 1.000000\n",
            "2023-04-23 02:45: **********Val Epoch 28: average Loss: 0.368826\n",
            "2023-04-23 02:45: *********************************Current best model saved!\n",
            "2023-04-23 02:45: Train Epoch 29: 0/161 Loss: 0.290459\n",
            "2023-04-23 02:45: Train Epoch 29: 20/161 Loss: 0.291922\n",
            "2023-04-23 02:45: Train Epoch 29: 40/161 Loss: 0.325935\n",
            "2023-04-23 02:45: Train Epoch 29: 60/161 Loss: 0.281003\n",
            "2023-04-23 02:45: Train Epoch 29: 80/161 Loss: 0.314260\n",
            "2023-04-23 02:45: Train Epoch 29: 100/161 Loss: 0.268268\n",
            "2023-04-23 02:45: Train Epoch 29: 120/161 Loss: 0.293025\n",
            "2023-04-23 02:45: Train Epoch 29: 140/161 Loss: 0.297702\n",
            "2023-04-23 02:45: Train Epoch 29: 160/161 Loss: 0.320725\n",
            "2023-04-23 02:45: **********Train Epoch 29: averaged Loss: 0.302975, tf_ratio: 1.000000\n",
            "2023-04-23 02:45: **********Val Epoch 29: average Loss: 0.366968\n",
            "2023-04-23 02:45: *********************************Current best model saved!\n",
            "2023-04-23 02:45: Train Epoch 30: 0/161 Loss: 0.284218\n",
            "2023-04-23 02:45: Train Epoch 30: 20/161 Loss: 0.274920\n",
            "2023-04-23 02:45: Train Epoch 30: 40/161 Loss: 0.328035\n",
            "2023-04-23 02:45: Train Epoch 30: 60/161 Loss: 0.319641\n",
            "2023-04-23 02:45: Train Epoch 30: 80/161 Loss: 0.283463\n",
            "2023-04-23 02:45: Train Epoch 30: 100/161 Loss: 0.308028\n",
            "2023-04-23 02:45: Train Epoch 30: 120/161 Loss: 0.294245\n",
            "2023-04-23 02:45: Train Epoch 30: 140/161 Loss: 0.323966\n",
            "2023-04-23 02:45: Train Epoch 30: 160/161 Loss: 0.303179\n",
            "2023-04-23 02:45: **********Train Epoch 30: averaged Loss: 0.304653, tf_ratio: 1.000000\n",
            "2023-04-23 02:45: **********Val Epoch 30: average Loss: 0.372525\n",
            "2023-04-23 02:45: Train Epoch 31: 0/161 Loss: 0.318734\n",
            "2023-04-23 02:45: Train Epoch 31: 20/161 Loss: 0.288381\n",
            "2023-04-23 02:45: Train Epoch 31: 40/161 Loss: 0.311688\n",
            "2023-04-23 02:45: Train Epoch 31: 60/161 Loss: 0.301416\n",
            "2023-04-23 02:45: Train Epoch 31: 80/161 Loss: 0.305129\n",
            "2023-04-23 02:45: Train Epoch 31: 100/161 Loss: 0.304933\n",
            "2023-04-23 02:45: Train Epoch 31: 120/161 Loss: 0.310421\n",
            "2023-04-23 02:45: Train Epoch 31: 140/161 Loss: 0.307478\n",
            "2023-04-23 02:45: Train Epoch 31: 160/161 Loss: 0.293127\n",
            "2023-04-23 02:45: **********Train Epoch 31: averaged Loss: 0.303206, tf_ratio: 1.000000\n",
            "2023-04-23 02:45: **********Val Epoch 31: average Loss: 0.378795\n",
            "2023-04-23 02:45: Train Epoch 32: 0/161 Loss: 0.309284\n",
            "2023-04-23 02:45: Train Epoch 32: 20/161 Loss: 0.299126\n",
            "2023-04-23 02:45: Train Epoch 32: 40/161 Loss: 0.297529\n",
            "2023-04-23 02:45: Train Epoch 32: 60/161 Loss: 0.290682\n",
            "2023-04-23 02:45: Train Epoch 32: 80/161 Loss: 0.310309\n",
            "2023-04-23 02:45: Train Epoch 32: 100/161 Loss: 0.312700\n",
            "2023-04-23 02:45: Train Epoch 32: 120/161 Loss: 0.270224\n",
            "2023-04-23 02:45: Train Epoch 32: 140/161 Loss: 0.293412\n",
            "2023-04-23 02:46: Train Epoch 32: 160/161 Loss: 0.272988\n",
            "2023-04-23 02:46: **********Train Epoch 32: averaged Loss: 0.296491, tf_ratio: 1.000000\n",
            "2023-04-23 02:46: **********Val Epoch 32: average Loss: 0.362632\n",
            "2023-04-23 02:46: *********************************Current best model saved!\n",
            "2023-04-23 02:46: Train Epoch 33: 0/161 Loss: 0.310820\n",
            "2023-04-23 02:46: Train Epoch 33: 20/161 Loss: 0.286186\n",
            "2023-04-23 02:46: Train Epoch 33: 40/161 Loss: 0.275405\n",
            "2023-04-23 02:46: Train Epoch 33: 60/161 Loss: 0.323976\n",
            "2023-04-23 02:46: Train Epoch 33: 80/161 Loss: 0.296716\n",
            "2023-04-23 02:46: Train Epoch 33: 100/161 Loss: 0.300128\n",
            "2023-04-23 02:46: Train Epoch 33: 120/161 Loss: 0.291547\n",
            "2023-04-23 02:46: Train Epoch 33: 140/161 Loss: 0.300318\n",
            "2023-04-23 02:46: Train Epoch 33: 160/161 Loss: 0.294803\n",
            "2023-04-23 02:46: **********Train Epoch 33: averaged Loss: 0.293751, tf_ratio: 1.000000\n",
            "2023-04-23 02:46: **********Val Epoch 33: average Loss: 0.375555\n",
            "2023-04-23 02:46: Train Epoch 34: 0/161 Loss: 0.272352\n",
            "2023-04-23 02:46: Train Epoch 34: 20/161 Loss: 0.268830\n",
            "2023-04-23 02:46: Train Epoch 34: 40/161 Loss: 0.291437\n",
            "2023-04-23 02:46: Train Epoch 34: 60/161 Loss: 0.283555\n",
            "2023-04-23 02:46: Train Epoch 34: 80/161 Loss: 0.283463\n",
            "2023-04-23 02:46: Train Epoch 34: 100/161 Loss: 0.278353\n",
            "2023-04-23 02:46: Train Epoch 34: 120/161 Loss: 0.304865\n",
            "2023-04-23 02:46: Train Epoch 34: 140/161 Loss: 0.301982\n",
            "2023-04-23 02:46: Train Epoch 34: 160/161 Loss: 0.288949\n",
            "2023-04-23 02:46: **********Train Epoch 34: averaged Loss: 0.288576, tf_ratio: 1.000000\n",
            "2023-04-23 02:46: **********Val Epoch 34: average Loss: 0.366522\n",
            "2023-04-23 02:46: Train Epoch 35: 0/161 Loss: 0.297739\n",
            "2023-04-23 02:46: Train Epoch 35: 20/161 Loss: 0.271286\n",
            "2023-04-23 02:46: Train Epoch 35: 40/161 Loss: 0.331516\n",
            "2023-04-23 02:46: Train Epoch 35: 60/161 Loss: 0.280602\n",
            "2023-04-23 02:46: Train Epoch 35: 80/161 Loss: 0.297457\n",
            "2023-04-23 02:46: Train Epoch 35: 100/161 Loss: 0.313000\n",
            "2023-04-23 02:46: Train Epoch 35: 120/161 Loss: 0.296962\n",
            "2023-04-23 02:46: Train Epoch 35: 140/161 Loss: 0.281323\n",
            "2023-04-23 02:46: Train Epoch 35: 160/161 Loss: 0.294210\n",
            "2023-04-23 02:46: **********Train Epoch 35: averaged Loss: 0.288361, tf_ratio: 1.000000\n",
            "2023-04-23 02:46: **********Val Epoch 35: average Loss: 0.368326\n",
            "2023-04-23 02:46: Train Epoch 36: 0/161 Loss: 0.305060\n",
            "2023-04-23 02:46: Train Epoch 36: 20/161 Loss: 0.306835\n",
            "2023-04-23 02:46: Train Epoch 36: 40/161 Loss: 0.268154\n",
            "2023-04-23 02:46: Train Epoch 36: 60/161 Loss: 0.275342\n",
            "2023-04-23 02:46: Train Epoch 36: 80/161 Loss: 0.280985\n",
            "2023-04-23 02:46: Train Epoch 36: 100/161 Loss: 0.274442\n",
            "2023-04-23 02:46: Train Epoch 36: 120/161 Loss: 0.300074\n",
            "2023-04-23 02:46: Train Epoch 36: 140/161 Loss: 0.278248\n",
            "2023-04-23 02:46: Train Epoch 36: 160/161 Loss: 0.295908\n",
            "2023-04-23 02:46: **********Train Epoch 36: averaged Loss: 0.283199, tf_ratio: 1.000000\n",
            "2023-04-23 02:46: **********Val Epoch 36: average Loss: 0.362856\n",
            "2023-04-23 02:46: Train Epoch 37: 0/161 Loss: 0.244983\n",
            "2023-04-23 02:46: Train Epoch 37: 20/161 Loss: 0.263848\n",
            "2023-04-23 02:47: Train Epoch 37: 40/161 Loss: 0.290955\n",
            "2023-04-23 02:47: Train Epoch 37: 60/161 Loss: 0.293120\n",
            "2023-04-23 02:47: Train Epoch 37: 80/161 Loss: 0.274938\n",
            "2023-04-23 02:47: Train Epoch 37: 100/161 Loss: 0.300462\n",
            "2023-04-23 02:47: Train Epoch 37: 120/161 Loss: 0.281747\n",
            "2023-04-23 02:47: Train Epoch 37: 140/161 Loss: 0.285710\n",
            "2023-04-23 02:47: Train Epoch 37: 160/161 Loss: 0.276700\n",
            "2023-04-23 02:47: **********Train Epoch 37: averaged Loss: 0.283418, tf_ratio: 1.000000\n",
            "2023-04-23 02:47: **********Val Epoch 37: average Loss: 0.373028\n",
            "2023-04-23 02:47: Train Epoch 38: 0/161 Loss: 0.294177\n",
            "2023-04-23 02:47: Train Epoch 38: 20/161 Loss: 0.280536\n",
            "2023-04-23 02:47: Train Epoch 38: 40/161 Loss: 0.296288\n",
            "2023-04-23 02:47: Train Epoch 38: 60/161 Loss: 0.298467\n",
            "2023-04-23 02:47: Train Epoch 38: 80/161 Loss: 0.287997\n",
            "2023-04-23 02:47: Train Epoch 38: 100/161 Loss: 0.282128\n",
            "2023-04-23 02:47: Train Epoch 38: 120/161 Loss: 0.257060\n",
            "2023-04-23 02:47: Train Epoch 38: 140/161 Loss: 0.269859\n",
            "2023-04-23 02:47: Train Epoch 38: 160/161 Loss: 0.276423\n",
            "2023-04-23 02:47: **********Train Epoch 38: averaged Loss: 0.279599, tf_ratio: 1.000000\n",
            "2023-04-23 02:47: **********Val Epoch 38: average Loss: 0.365442\n",
            "2023-04-23 02:47: Train Epoch 39: 0/161 Loss: 0.278668\n",
            "2023-04-23 02:47: Train Epoch 39: 20/161 Loss: 0.280205\n",
            "2023-04-23 02:47: Train Epoch 39: 40/161 Loss: 0.290706\n",
            "2023-04-23 02:47: Train Epoch 39: 60/161 Loss: 0.299816\n",
            "2023-04-23 02:47: Train Epoch 39: 80/161 Loss: 0.279999\n",
            "2023-04-23 02:47: Train Epoch 39: 100/161 Loss: 0.269224\n",
            "2023-04-23 02:47: Train Epoch 39: 120/161 Loss: 0.253654\n",
            "2023-04-23 02:47: Train Epoch 39: 140/161 Loss: 0.267848\n",
            "2023-04-23 02:47: Train Epoch 39: 160/161 Loss: 0.283016\n",
            "2023-04-23 02:47: **********Train Epoch 39: averaged Loss: 0.273891, tf_ratio: 1.000000\n",
            "2023-04-23 02:47: **********Val Epoch 39: average Loss: 0.365159\n",
            "2023-04-23 02:47: Train Epoch 40: 0/161 Loss: 0.260708\n",
            "2023-04-23 02:47: Train Epoch 40: 20/161 Loss: 0.289770\n",
            "2023-04-23 02:47: Train Epoch 40: 40/161 Loss: 0.296534\n",
            "2023-04-23 02:47: Train Epoch 40: 60/161 Loss: 0.262849\n",
            "2023-04-23 02:47: Train Epoch 40: 80/161 Loss: 0.265300\n",
            "2023-04-23 02:47: Train Epoch 40: 100/161 Loss: 0.232937\n",
            "2023-04-23 02:47: Train Epoch 40: 120/161 Loss: 0.268731\n",
            "2023-04-23 02:47: Train Epoch 40: 140/161 Loss: 0.243642\n",
            "2023-04-23 02:47: Train Epoch 40: 160/161 Loss: 0.265132\n",
            "2023-04-23 02:47: **********Train Epoch 40: averaged Loss: 0.271977, tf_ratio: 1.000000\n",
            "2023-04-23 02:47: **********Val Epoch 40: average Loss: 0.373109\n",
            "2023-04-23 02:47: Train Epoch 41: 0/161 Loss: 0.263530\n",
            "2023-04-23 02:47: Train Epoch 41: 20/161 Loss: 0.278859\n",
            "2023-04-23 02:47: Train Epoch 41: 40/161 Loss: 0.267689\n",
            "2023-04-23 02:47: Train Epoch 41: 60/161 Loss: 0.269688\n",
            "2023-04-23 02:47: Train Epoch 41: 80/161 Loss: 0.268683\n",
            "2023-04-23 02:47: Train Epoch 41: 100/161 Loss: 0.288621\n",
            "2023-04-23 02:48: Train Epoch 41: 120/161 Loss: 0.254882\n",
            "2023-04-23 02:48: Train Epoch 41: 140/161 Loss: 0.300392\n",
            "2023-04-23 02:48: Train Epoch 41: 160/161 Loss: 0.264067\n",
            "2023-04-23 02:48: **********Train Epoch 41: averaged Loss: 0.270544, tf_ratio: 1.000000\n",
            "2023-04-23 02:48: **********Val Epoch 41: average Loss: 0.363982\n",
            "2023-04-23 02:48: Train Epoch 42: 0/161 Loss: 0.284041\n",
            "2023-04-23 02:48: Train Epoch 42: 20/161 Loss: 0.265705\n",
            "2023-04-23 02:48: Train Epoch 42: 40/161 Loss: 0.249924\n",
            "2023-04-23 02:48: Train Epoch 42: 60/161 Loss: 0.281828\n",
            "2023-04-23 02:48: Train Epoch 42: 80/161 Loss: 0.274096\n",
            "2023-04-23 02:48: Train Epoch 42: 100/161 Loss: 0.273205\n",
            "2023-04-23 02:48: Train Epoch 42: 120/161 Loss: 0.253107\n",
            "2023-04-23 02:48: Train Epoch 42: 140/161 Loss: 0.276604\n",
            "2023-04-23 02:48: Train Epoch 42: 160/161 Loss: 0.284248\n",
            "2023-04-23 02:48: **********Train Epoch 42: averaged Loss: 0.267110, tf_ratio: 1.000000\n",
            "2023-04-23 02:48: **********Val Epoch 42: average Loss: 0.372942\n",
            "2023-04-23 02:48: Train Epoch 43: 0/161 Loss: 0.285011\n",
            "2023-04-23 02:48: Train Epoch 43: 20/161 Loss: 0.267216\n",
            "2023-04-23 02:48: Train Epoch 43: 40/161 Loss: 0.279584\n",
            "2023-04-23 02:48: Train Epoch 43: 60/161 Loss: 0.256633\n",
            "2023-04-23 02:48: Train Epoch 43: 80/161 Loss: 0.237077\n",
            "2023-04-23 02:48: Train Epoch 43: 100/161 Loss: 0.259718\n",
            "2023-04-23 02:48: Train Epoch 43: 120/161 Loss: 0.252216\n",
            "2023-04-23 02:48: Train Epoch 43: 140/161 Loss: 0.261880\n",
            "2023-04-23 02:48: Train Epoch 43: 160/161 Loss: 0.265135\n",
            "2023-04-23 02:48: **********Train Epoch 43: averaged Loss: 0.261248, tf_ratio: 1.000000\n",
            "2023-04-23 02:48: **********Val Epoch 43: average Loss: 0.365705\n",
            "2023-04-23 02:48: Train Epoch 44: 0/161 Loss: 0.256730\n",
            "2023-04-23 02:48: Train Epoch 44: 20/161 Loss: 0.281247\n",
            "2023-04-23 02:48: Train Epoch 44: 40/161 Loss: 0.273551\n",
            "2023-04-23 02:48: Train Epoch 44: 60/161 Loss: 0.263879\n",
            "2023-04-23 02:48: Train Epoch 44: 80/161 Loss: 0.294916\n",
            "2023-04-23 02:48: Train Epoch 44: 100/161 Loss: 0.254028\n",
            "2023-04-23 02:48: Train Epoch 44: 120/161 Loss: 0.254719\n",
            "2023-04-23 02:48: Train Epoch 44: 140/161 Loss: 0.273078\n",
            "2023-04-23 02:48: Train Epoch 44: 160/161 Loss: 0.252565\n",
            "2023-04-23 02:48: **********Train Epoch 44: averaged Loss: 0.261294, tf_ratio: 1.000000\n",
            "2023-04-23 02:48: **********Val Epoch 44: average Loss: 0.368736\n",
            "2023-04-23 02:48: Train Epoch 45: 0/161 Loss: 0.242423\n",
            "2023-04-23 02:48: Train Epoch 45: 20/161 Loss: 0.265317\n",
            "2023-04-23 02:48: Train Epoch 45: 40/161 Loss: 0.294915\n",
            "2023-04-23 02:48: Train Epoch 45: 60/161 Loss: 0.248226\n",
            "2023-04-23 02:48: Train Epoch 45: 80/161 Loss: 0.237612\n",
            "2023-04-23 02:48: Train Epoch 45: 100/161 Loss: 0.245224\n",
            "2023-04-23 02:48: Train Epoch 45: 120/161 Loss: 0.255506\n",
            "2023-04-23 02:48: Train Epoch 45: 140/161 Loss: 0.271012\n",
            "2023-04-23 02:48: Train Epoch 45: 160/161 Loss: 0.230310\n",
            "2023-04-23 02:48: **********Train Epoch 45: averaged Loss: 0.259749, tf_ratio: 1.000000\n",
            "2023-04-23 02:49: **********Val Epoch 45: average Loss: 0.375260\n",
            "2023-04-23 02:49: Train Epoch 46: 0/161 Loss: 0.241297\n",
            "2023-04-23 02:49: Train Epoch 46: 20/161 Loss: 0.240255\n",
            "2023-04-23 02:49: Train Epoch 46: 40/161 Loss: 0.237495\n",
            "2023-04-23 02:49: Train Epoch 46: 60/161 Loss: 0.271738\n",
            "2023-04-23 02:49: Train Epoch 46: 80/161 Loss: 0.225886\n",
            "2023-04-23 02:49: Train Epoch 46: 100/161 Loss: 0.244197\n",
            "2023-04-23 02:49: Train Epoch 46: 120/161 Loss: 0.268265\n",
            "2023-04-23 02:49: Train Epoch 46: 140/161 Loss: 0.247157\n",
            "2023-04-23 02:49: Train Epoch 46: 160/161 Loss: 0.256754\n",
            "2023-04-23 02:49: **********Train Epoch 46: averaged Loss: 0.255838, tf_ratio: 1.000000\n",
            "2023-04-23 02:49: **********Val Epoch 46: average Loss: 0.364990\n",
            "2023-04-23 02:49: Train Epoch 47: 0/161 Loss: 0.248041\n",
            "2023-04-23 02:49: Train Epoch 47: 20/161 Loss: 0.266052\n",
            "2023-04-23 02:49: Train Epoch 47: 40/161 Loss: 0.259378\n",
            "2023-04-23 02:49: Train Epoch 47: 60/161 Loss: 0.260720\n",
            "2023-04-23 02:49: Train Epoch 47: 80/161 Loss: 0.269042\n",
            "2023-04-23 02:49: Train Epoch 47: 100/161 Loss: 0.233076\n",
            "2023-04-23 02:49: Train Epoch 47: 120/161 Loss: 0.260532\n",
            "2023-04-23 02:49: Train Epoch 47: 140/161 Loss: 0.238454\n",
            "2023-04-23 02:49: Train Epoch 47: 160/161 Loss: 0.240379\n",
            "2023-04-23 02:49: **********Train Epoch 47: averaged Loss: 0.254787, tf_ratio: 1.000000\n",
            "2023-04-23 02:49: **********Val Epoch 47: average Loss: 0.375307\n",
            "2023-04-23 02:49: Validation performance didn't improve for 15 epochs. Training stops.\n",
            "2023-04-23 02:49: Total training time: 10.7956min, best loss: 0.362632\n",
            "2023-04-23 02:49: Saving current best model to /content/gdrive/MyDrive/Models/AGCRN/experiments/Speed20230423023839/best_model.pth\n",
            "2023-04-23 02:49: Horizon 01, MAE: 0.20, RMSE: 0.64, MAPE: 0.3938%\n",
            "2023-04-23 02:49: Horizon 02, MAE: 0.22, RMSE: 0.66, MAPE: 0.4140%\n",
            "2023-04-23 02:49: Horizon 03, MAE: 0.26, RMSE: 0.70, MAPE: 0.4901%\n",
            "2023-04-23 02:49: Horizon 04, MAE: 0.31, RMSE: 0.76, MAPE: 0.5934%\n",
            "2023-04-23 02:49: Horizon 05, MAE: 0.38, RMSE: 0.85, MAPE: 0.7180%\n",
            "2023-04-23 02:49: Horizon 06, MAE: 0.45, RMSE: 0.95, MAPE: 0.8543%\n",
            "2023-04-23 02:49: Horizon 07, MAE: 0.53, RMSE: 1.07, MAPE: 1.0017%\n",
            "2023-04-23 02:49: Horizon 08, MAE: 0.62, RMSE: 1.19, MAPE: 1.1537%\n",
            "2023-04-23 02:49: Horizon 09, MAE: 0.70, RMSE: 1.33, MAPE: 1.3131%\n",
            "2023-04-23 02:49: Horizon 10, MAE: 0.79, RMSE: 1.47, MAPE: 1.4747%\n",
            "2023-04-23 02:49: Horizon 11, MAE: 0.88, RMSE: 1.61, MAPE: 1.6422%\n",
            "2023-04-23 02:49: Horizon 12, MAE: 0.97, RMSE: 1.77, MAPE: 1.8148%\n",
            "2023-04-23 02:49: Average Horizon, MAE: 0.53, RMSE: 1.14, MAPE: 0.9886%\n"
          ]
        }
      ]
    }
  ]
}